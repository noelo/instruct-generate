{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install docling quackling llama-index llama-index-llms-openllm\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import PipelineOptions\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/noelo/dev/instruct-injest/data/CELEX_32021R1173_EN_TXT.pdf\"\n",
    "converter = DocumentConverter(pipeline_options=PipelineOptions())\n",
    "result = converter.convert_single(source)\n",
    "print(len(result.pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "print(os.getenv(\"API_KEY\"))\n",
    "print(os.getenv(\"LLM_URL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quackling.llama_index.node_parsers import HierarchicalJSONNodeParser\n",
    "from quackling.llama_index.readers import DoclingPDFReader\n",
    "\n",
    "reader = DoclingPDFReader(parse_type=DoclingPDFReader.ParseType.JSON)\n",
    "node_parser = HierarchicalJSONNodeParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reader.load_data(file_path=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "pprint(docs, max_length=2, max_string=250, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[node_parser],\n",
    ")\n",
    "nodes = pipeline.run(documents=docs)\n",
    "print(len(nodes))\n",
    "# for x in nodes:\n",
    "#     print(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text =''\n",
    "for x in nodes:\n",
    "    raw_text += x.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('nomic-ai/nomic-embed-text-v1.5')\n",
    "encoded_output = tokenizer(raw_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoded_output.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openllm import OpenLLM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field, constr\n",
    "\n",
    "\n",
    "class QuestionsAndAnswer(BaseModel):\n",
    "    question: constr(min_length=1)\n",
    "    answer: constr(min_length=1)\n",
    "\n",
    "\n",
    "class SeedExample(BaseModel):\n",
    "    context: constr(min_length=1)\n",
    "    questions_and_answers: List[QuestionsAndAnswer] = Field(\n",
    "        ..., min_items=3, set=True\n",
    "    )\n",
    "\n",
    "\n",
    "class QNAModel(BaseModel):\n",
    "    created_by: Optional[constr(min_length=1)] = None\n",
    "    domain: Optional[constr(min_length=1)] = Field(\n",
    "        None, examples=['Chemistry', 'History', 'Pop culture']\n",
    "    )\n",
    "    seed_examples: Optional[List[SeedExample]] = Field(\n",
    "        None, min_items=5, set=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompt=f\"you are a helpful writing assistant. Given the following context generate 30 question and answer pairs. Ensure that the questions can be answered by the context given.  Group the questions into different semantic contexts. Also output a description of the contexts. You MUST answer using the following json schema: {QNAModel.schema_json()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenLLM(\n",
    "    model=\"mistral-7b-instruct\", api_base=os.getenv(\"LLM_URL\"),\n",
    "    api_key=os.getenv(\"API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "# completion_response = llm.complete(gen_prompt+raw_text[:5000],max_tokens=1000,timeout=120.0)\n",
    "# print(completion_response)\n",
    "\n",
    "comp_resp=''\n",
    "\n",
    "for it in llm.stream_complete(gen_prompt+raw_text[:5000],max_tokens=4000,timeout=120.0):\n",
    "    comp_resp+=it.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip -q install semantic-router semantic-chunkers\n",
    "\n",
    "from semantic_router.encoders.fastembed import FastEmbedEncoder\n",
    "\n",
    "encoder = FastEmbedEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from semantic_router.splitters import RollingWindowSplitter\n",
    "from semantic_router.utils.logger import logger\n",
    "\n",
    "logger.setLevel(\"WARNING\")  # reduce logs from splitter\n",
    "\n",
    "splitter = RollingWindowSplitter(\n",
    "    encoder=encoder,\n",
    "    dynamic_threshold=True,\n",
    "    min_split_tokens=100,\n",
    "    max_split_tokens=4000,\n",
    "    # window_size=2,\n",
    "    plot_splits=True,  # set this to true to visualize chunking\n",
    "    enable_statistics=True  # to print chunking stats\n",
    ")\n",
    "     \n",
    "\n",
    "splits = splitter([result.output.export_to_markdown()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "\n",
    "chunker = StatisticalChunker(encoder=encoder,enable_statistics=True,plot_chunks=False,min_split_tokens=300, max_split_tokens=2000)\n",
    "\n",
    "chunks = chunker(docs=[result.output.export_to_markdown()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker.print(chunks[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
