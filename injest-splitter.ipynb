{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install onnxruntime==1.19.2\n",
    "%pip install fastembed\n",
    "%pip -q install docling quackling llama-index llama-index-llms-openllm pydantic-yaml\n",
    "%pip -q install semantic-router semantic-chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import PipelineOptions\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from semantic_router.encoders.fastembed import FastEmbedEncoder\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from __future__ import annotations\n",
    "from typing import Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_core import from_json\n",
    "from pydantic import ValidationError\n",
    "from pydantic_yaml import to_yaml_str\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/noelo/dev/instruct-injest/data/CELEX_32021R1173_EN_TXT.pdf\"\n",
    "converter = DocumentConverter(pipeline_options=PipelineOptions(do_ocr=False, do_table_structure=False))\n",
    "result = converter.convert_single(source)\n",
    "_log.info(len(result.pages))\n",
    "raw_text = result.output.export_to_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This area needs work. It's not clear from the docs if the context passed to InstructLab can be a summerization of the actual text from the knowledge document.\n",
    "Depending the answer the max_split_tokens value may need to change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_MAX_SPLIT_TOKENS=500\n",
    "encoder = FastEmbedEncoder()\n",
    "chunker = StatisticalChunker(encoder=encoder,enable_statistics=True,plot_chunks=True,min_split_tokens=200, max_split_tokens=CONTEXT_MAX_SPLIT_TOKENS)\n",
    "%pip show onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunker(docs=[raw_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.setLevel(level=logging.INFO)\n",
    "\n",
    "llm_base = OpenLLM(\n",
    "    model=os.getenv(\"MODEL_NAME\"), \n",
    "    api_base=os.getenv(\"LLM_URL\"),\n",
    "    api_key=os.getenv(\"API_KEY\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Notes\n",
    "\n",
    "1. Does the answers for the questions have to come from the actual context in the file or can the context be a summarization of the info that's in the knowledge markdown files\n",
    "Every fact should be supported by the context, but the answers do not need to be verbatim.\n",
    "\n",
    "2. The docs say that \"Each qna.yaml file needs at least three question and answer pairs per context chunk with a maximum token count of 250 tokens.\". Is that 250 tokens per context or per question and answer pair?\n",
    "The 250 is an approximate number based on the maximum total size for SDG. The total tokens of Context + 3 Q&A must be less than 750 tokens. To have enough data for a context to answer the questions, an approximate 500 tokens are recommended for context, and the remaining 250 for the 3 Q&A.\n",
    "At the end, the Q&A length is no problem as long as the context+3 Q&As remain < 750\n",
    "\n",
    "3. Also from the docs, \"Each qna.yaml needs five context blocks and has a maximum token count of 500 tokens.\" Is that per context or for all contexts?\n",
    "This is per context, and the recommended 500 is to ensure there is enough data in the context to answer the questions. It can be less or it can be more, as long as the final lenght of Context + 3 Q&A < 750 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS_CONTEXT=500\n",
    "MAX_TOKENS_QNA=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAndAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class SeedExample(BaseModel):\n",
    "    context: Annotated[str, Field(None,max_length=500)]\n",
    "    questions_and_answers: List[QuestionAndAnswer] = Field(None, min_items=3, set=True)\n",
    "\n",
    "class QNAModel(BaseModel):\n",
    "    version: Annotated[int,Field(3)]\n",
    "    created_by: Annotated[str, Field(None)]\n",
    "    domain: Annotated[str, Field(None)]\n",
    "    seed_examples: Annotated[List[SeedExample], Field(None, min_items=5, set=True)]\n",
    "\n",
    "print(QNAModel.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompt=f\"You are a helpful question and answer writing assistant. Given the following information generate 1 seed examples containing 3 question and answer pairs. Ensure that the questions can be answered by the information given. Do not number the pairs.  All output MUST be in valid JSON format.\\n\\nInformation:\"\n",
    "\n",
    "json_prompt=f\"\\n\\nHere's a JSON schema to follow: {SeedExample.model_json_schema()}.\\n\\nOutput a valid JSON object but do not repeat the schema.\"\n",
    "\n",
    "for ch in chunks[0]:\n",
    "\n",
    "    llm_msg = gen_prompt+ch.content+json_prompt\n",
    "    _log.debug(llm_msg)\n",
    "\n",
    "    it = llm_base.complete(llm_msg,max_tokens=MAX_TOKENS_QNA,timeout=120.0)\n",
    "    \n",
    "    # Ensure that we just take the json output, sometimes we get some rubbish upfront\n",
    "    json_start = it.text.find('{')\n",
    "    extracted_json = it.text[json_start:]\n",
    "\n",
    "\n",
    "    qna_list=[]\n",
    "    try:\n",
    "        res = SeedExample.model_validate(from_json(extracted_json,allow_partial=True,cache_strings='keys'))\n",
    "        res.context=ch.content\n",
    "        yml = to_yaml_str(res)\n",
    "        print(yml)\n",
    "    except (ValidationError ,ValueError) as e:\n",
    "        _log.error(e,extracted_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
