{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "types-requests 2.32.0.20240907 requires urllib3>=2, but you have urllib3 1.26.20 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepsearch-toolkit 1.0.0 requires urllib3<2.0.0,>=1.26.8, but you have urllib3 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install docling quackling llama-index llama-index-llms-openllm\n",
    "%pip -q install semantic-router semantic-chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noelo/.pyenv/versions/3.10.14/envs/instvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import PipelineOptions\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from semantic_router.encoders.fastembed import FastEmbedEncoder\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from __future__ import annotations\n",
    "from typing import Annotated, List, Optional\n",
    "from pydantic import BaseModel, Field, constr\n",
    "from pydantic_core import from_json\n",
    "from pydantic import ValidationError\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 27750.59it/s]\n",
      "/home/noelo/.pyenv/versions/3.10.14/envs/instvenv/lib/python3.10/site-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "/home/noelo/.pyenv/versions/3.10.14/envs/instvenv/lib/python3.10/site-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "/home/noelo/.pyenv/versions/3.10.14/envs/instvenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.document_converter:Processing document CELEX_32021R1173_EN_TXT.pdf\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.627\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.880\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.928\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.668\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.399\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.406\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.403\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.260\n",
      "INFO:docling.document_converter:Finished converting page batch time=3.850\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.375\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.117\n",
      "INFO:docling.document_converter:Finished converting page batch time=4.234\n",
      "INFO:docling.document_converter:Finished converting page batch time=0.912\n",
      "INFO:docling.document_converter:Finished converting document time-pages=54.46/49\n",
      "INFO:__main__:49\n"
     ]
    }
   ],
   "source": [
    "source = \"/home/noelo/dev/instruct-injest/data/CELEX_32021R1173_EN_TXT.pdf\"\n",
    "converter = DocumentConverter(pipeline_options=PipelineOptions())\n",
    "result = converter.convert_single(source)\n",
    "_log.info(len(result.pages))\n",
    "raw_text = result.output.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAndAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class SeedExample(BaseModel):\n",
    "    context: str\n",
    "    questions_and_answers: List[QuestionAndAnswer] = Field(None, min_items=3, set=True)\n",
    "\n",
    "class QNAModel(BaseModel):\n",
    "    created_by: Annotated[str, Field(None)]\n",
    "    domain: Annotated[str, Field(None)]\n",
    "    seed_examples: Annotated[List[SeedExample], Field(None, min_items=5, set=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 13107.20it/s]\n",
      "\u001b[32m2024-09-11 14:18:41 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 2000. Splitting to sentences before semantically merging.\u001b[0m\n",
      "  5%|▌         | 1/19 [00:00<00:17,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 64\n",
      "  - Total Chunks: 6\n",
      "  - Chunks by Threshold: 5\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 210\n",
      "  - Maximum Token Size of Chunk: 653\n",
      "  - Similarity Chunk Ratio: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/19 [00:02<00:17,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 69\n",
      "  - Total Chunks: 7\n",
      "  - Chunks by Threshold: 6\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 120\n",
      "  - Maximum Token Size of Chunk: 686\n",
      "  - Similarity Chunk Ratio: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3/19 [00:03<00:16,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 66\n",
      "  - Total Chunks: 5\n",
      "  - Chunks by Threshold: 4\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 87\n",
      "  - Maximum Token Size of Chunk: 941\n",
      "  - Similarity Chunk Ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4/19 [00:04<00:18,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 65\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 413\n",
      "  - Maximum Token Size of Chunk: 1119\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 5/19 [00:06<00:19,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 85\n",
      "  - Total Chunks: 5\n",
      "  - Chunks by Threshold: 4\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 388\n",
      "  - Maximum Token Size of Chunk: 941\n",
      "  - Similarity Chunk Ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6/19 [00:07<00:18,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 73\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 344\n",
      "  - Maximum Token Size of Chunk: 716\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7/19 [00:09<00:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 78\n",
      "  - Total Chunks: 6\n",
      "  - Chunks by Threshold: 5\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 43\n",
      "  - Maximum Token Size of Chunk: 535\n",
      "  - Similarity Chunk Ratio: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8/19 [00:09<00:13,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 66\n",
      "  - Total Chunks: 5\n",
      "  - Chunks by Threshold: 4\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 229\n",
      "  - Maximum Token Size of Chunk: 453\n",
      "  - Similarity Chunk Ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 9/19 [00:11<00:12,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 70\n",
      "  - Total Chunks: 5\n",
      "  - Chunks by Threshold: 4\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 312\n",
      "  - Maximum Token Size of Chunk: 445\n",
      "  - Similarity Chunk Ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 10/19 [00:12<00:10,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 76\n",
      "  - Total Chunks: 5\n",
      "  - Chunks by Threshold: 4\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 106\n",
      "  - Maximum Token Size of Chunk: 455\n",
      "  - Similarity Chunk Ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 11/19 [00:12<00:08,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 70\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 99\n",
      "  - Maximum Token Size of Chunk: 539\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 12/19 [00:13<00:07,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 69\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 177\n",
      "  - Maximum Token Size of Chunk: 666\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13/19 [00:15<00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 74\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 335\n",
      "  - Maximum Token Size of Chunk: 703\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 14/19 [00:16<00:06,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 78\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 368\n",
      "  - Maximum Token Size of Chunk: 596\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 15/19 [00:18<00:05,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 75\n",
      "  - Total Chunks: 5\n",
      "  - Chunks by Threshold: 4\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 108\n",
      "  - Maximum Token Size of Chunk: 619\n",
      "  - Similarity Chunk Ratio: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 16/19 [00:19<00:03,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 68\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 193\n",
      "  - Maximum Token Size of Chunk: 775\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 17/19 [00:20<00:02,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 71\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 161\n",
      "  - Maximum Token Size of Chunk: 453\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 18/19 [00:21<00:01,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 69\n",
      "  - Total Chunks: 4\n",
      "  - Chunks by Threshold: 3\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 238\n",
      "  - Maximum Token Size of Chunk: 664\n",
      "  - Similarity Chunk Ratio: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:22<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Statistics:\n",
      "  - Total Documents: 74\n",
      "  - Total Chunks: 3\n",
      "  - Chunks by Threshold: 2\n",
      "  - Chunks by Max Chunk Size: 0\n",
      "  - Last Chunk: 1\n",
      "  - Minimum Token Size of Chunk: 203\n",
      "  - Maximum Token Size of Chunk: 583\n",
      "  - Similarity Chunk Ratio: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = FastEmbedEncoder()\n",
    "chunker = StatisticalChunker(encoder=encoder,enable_statistics=True,plot_chunks=False,min_split_tokens=300, max_split_tokens=2000)\n",
    "chunks = chunker(docs=[raw_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.setLevel(level=logging.INFO)\n",
    "\n",
    "llm_base = OpenLLM(\n",
    "    model=os.getenv(\"MODEL_NAME\"), \n",
    "    api_base=os.getenv(\"LLM_URL\"),\n",
    "    api_key=os.getenv(\"API_KEY\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompt=f\"You are a helpful question and answer writing assistant. Given the following context generate 5 seed examples containing 3 question and answer pairs. Ensure that the questions can be answered by the context given. Do not number the pairs. Also generate a description of the contexts. All output MUST be in valid JSON format.\\n\\nContext:\"\n",
    "\n",
    "json_prompt=f\"\\n\\nHere's a JSON schema to follow: {QNAModel.model_json_schema()}.\\n\\nOutput a valid JSON object but do not repeat the schema.\"\n",
    "\n",
    "for ch in chunks[0]:\n",
    "    llm_msg = gen_prompt+ch.content+json_prompt\n",
    "    _log.debug(llm_msg)\n",
    "\n",
    "    it = llm_base.complete(llm_msg,max_tokens=4000,timeout=120.0)\n",
    "    \n",
    "    # Ensure that we just take the json output, sometimes we get some rubbish upfront\n",
    "    json_start = it.text.find('{')\n",
    "    extracted_json = it.text[json_start:]\n",
    "\n",
    "    qna_list=[]\n",
    "    try:\n",
    "        res = QNAModel.model_validate(from_json(extracted_json,allow_partial=True,cache_strings='keys'))\n",
    "        print(type(res),id(res))\n",
    "        qna_list = qna_list + [res]\n",
    "        print(len(qna_list))\n",
    "    except (ValidationError ,ValueError) as e:\n",
    "        _log.error(e,extracted_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
